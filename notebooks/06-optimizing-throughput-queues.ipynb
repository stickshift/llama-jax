{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Throughput: Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown the average throughput of llama-jax to be 10 tps short of Ollama (25 vs. 35 running on 64GB M1 MBP). The goal of this series of optimization experiments is to see how closely we can match Ollama's throughput.\n",
    "\n",
    "## Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-05 16:46:02,657:jax._src.xla_bridge:1000: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1741211162.658151 15159954 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1741211162.666866 15159954 service.cc:145] XLA service 0x12b3eb5f0 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1741211162.666877 15159954 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1741211162.668319 15159954 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1741211162.668331 15159954 mps_client.cc:384] XLA backend will use up to 51539214336 bytes on device 0 for SimpleAllocator.\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Callable, Iterator, Sequence\n",
    "from concurrent.futures import wait,ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from queue import SimpleQueue\n",
    "from time import time_ns as seed\n",
    "\n",
    "import jax\n",
    "from jax import Array, numpy as jnp, random\n",
    "from jax.nn import softmax\n",
    "import ollama\n",
    "from pandas import DataFrame\n",
    "from pydantic import BaseModel\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import llama_jax as ll\n",
    "from llama_jax.chat import Message\n",
    "from llama_jax.checkpoint import ModelConfig\n",
    "from llama_jax.model import Model\n",
    "from llama_jax.kvc import KVCache\n",
    "from llama_jax.tools import default_arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prompts = 30\n",
    "max_tokens = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can Math Explain Reality?',\n",
       " 'What is Perfectly Rational Beauty?',\n",
       " 'Do Geometric Shapes Hold Secrets?',\n",
       " 'How Do Stars Align Physics?',\n",
       " 'Is Symmetry a Universal Law?',\n",
       " 'Do Laws of Nature Vary?',\n",
       " 'Why Does Time Work Differently?',\n",
       " 'Are Physical Constants Fixed?',\n",
       " 'How Do Black Holes Form?',\n",
       " 'Does Space-Time Matter More?',\n",
       " 'What is the Speed Limit?',\n",
       " 'Can Machines Solve Math Problems?',\n",
       " 'Is Pi an Infinite Answer?',\n",
       " 'Can Numbers Describe Reality?',\n",
       " 'Do Fractals Hold Secrets?',\n",
       " 'How Does Energy Transfer?',\n",
       " 'Is Gravity a Force?',\n",
       " 'Do Waves Have Speed Limits?',\n",
       " 'Why Do Some Laws Fail?',\n",
       " 'Do All Objects Decay Slowly?',\n",
       " 'How Much Energy is Lost?',\n",
       " 'What is the Speed of Light?',\n",
       " 'Can Machines See Color?',\n",
       " 'Why Do Plants Grow Taller?',\n",
       " 'Are Numbers Always Counting?',\n",
       " 'Does Matter Have a Speed Limit?',\n",
       " 'Can We Control Gravity?',\n",
       " 'Is Time Travel Possible?',\n",
       " 'Can Machines Learn Math Faster?',\n",
       " 'Do All Physical Laws Apply?',\n",
       " 'How Does Weather Affect Physics?',\n",
       " 'Is the Universe Made of Matter?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PromptPool(BaseModel):\n",
    "    prompts: list[str]\n",
    "\n",
    "prompt = ll.tools.prompt(\n",
    "    f\"\"\"\n",
    "    Generate {n_prompts} LLM prompts. Each prompt should pose an interesting question about math and science in 3 to 6 words.\n",
    "    Your response must be formatted as JSON.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\", \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "    format=PromptPool.model_json_schema(),\n",
    ")\n",
    "\n",
    "prompt_pool = PromptPool.model_validate_json(response.message.content)\n",
    "prompts = prompt_pool.prompts\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_prompt = \"Why is the sky blue?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline = Callable[[Sequence[str]], Iterator[str]]\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=\"config\")\n",
    "def transform(\n",
    "    config: ModelConfig,\n",
    "    state: Model,\n",
    "    token_ids: Array,\n",
    "    position_mask: Array,\n",
    "    *,\n",
    "    kvc: KVCache | None = None,\n",
    ") -> Array | tuple[Array, KVCache]:\n",
    "    \"\"\"Transform token_ids into next token logits.\"\"\"\n",
    "\n",
    "    # Remember if cache was provided\n",
    "    external_cache = kvc is not None\n",
    "\n",
    "    # Defaults\n",
    "    kvc = default_arg(kvc, default_factory=partial(ll.kvc.create, config))\n",
    "\n",
    "    # Sanity check\n",
    "    assert token_ids.ndim == 2\n",
    "\n",
    "    # Map tokens to embeddings\n",
    "    x = ll.embeddings.forward(config, state.embeddings, token_ids)\n",
    "\n",
    "    # Create mask\n",
    "    mask = ll.attention.attention_mask(config, position_mask)\n",
    "\n",
    "    # Create mutable kv cache\n",
    "    kvc_layers = list(kvc)\n",
    "\n",
    "    # Apply layers\n",
    "    for i, layer in enumerate(state.layers):\n",
    "        x, kvc_layers[i] = ll.layer.forward(config, layer, state.rope, mask, x, kvc_layers[i])\n",
    "\n",
    "    # Convert kv caches back into immutable sequence\n",
    "    kvc = KVCache(kvc_layers)\n",
    "\n",
    "    # Apply head\n",
    "    x = ll.head.forward(config, state.head, x, position_mask)\n",
    "\n",
    "    # Return updated cache if provided\n",
    "    if external_cache:\n",
    "        return x, kvc\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"temperature\", \"top_k\", \"top_p\"))\n",
    "def next_token_id(\n",
    "    logits: Array,\n",
    "    *,\n",
    "    key: Array | None = None,\n",
    "    temperature: float | None = None,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    ") -> Array:\n",
    "    \"\"\"Select next token id using temperature, top k, and top p sampling.\"\"\"\n",
    "\n",
    "    # Temperature\n",
    "    # -----------\n",
    "\n",
    "    # Defaults\n",
    "    temperature = default_arg(temperature, 0.6)\n",
    "\n",
    "    # If temperature is 0, return the top token\n",
    "    if temperature == 0:\n",
    "        return jnp.argmax(logits, axis=-1, keepdims=True)\n",
    "\n",
    "    # Apply temperature\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Ranking\n",
    "    # -------\n",
    "\n",
    "    # Sort logits in descending order, maintaining original indices\n",
    "    indices = jnp.argsort(logits, axis=-1, descending=True)\n",
    "\n",
    "    # Top K\n",
    "    # -----\n",
    "\n",
    "    # Defaults\n",
    "    top_k = default_arg(top_k, 50)\n",
    "\n",
    "    # Apply top k to entire batch at once\n",
    "    indices = indices[:, :top_k]\n",
    "    logits = jnp.take_along_axis(logits, indices, axis=-1)\n",
    "\n",
    "    # Top P\n",
    "    # -----\n",
    "\n",
    "    # Defaults\n",
    "    top_p = default_arg(top_p, 0.9)\n",
    "\n",
    "    # Convert remaining logits to probabilities\n",
    "    probs = softmax(logits, axis=-1)\n",
    "\n",
    "    # Find index where cumulative sum of probs exceeds p\n",
    "    cumulative_mask = probs.cumsum(axis=-1) <= top_p\n",
    "    cutoff = jnp.sum(cumulative_mask, axis=-1, keepdims=True)\n",
    "\n",
    "    # Calculate mask for indicies <= cutoff\n",
    "    mask = jnp.broadcast_to(jnp.arange(logits.shape[-1]), logits.shape) <= cutoff\n",
    "\n",
    "    # Zero out logits above cutoff\n",
    "    logits = jnp.where(mask, logits, 0)\n",
    "\n",
    "    # Random Selection\n",
    "    # ----------------\n",
    "\n",
    "    assert key is not None\n",
    "\n",
    "    # Randomly choose from remaining logits\n",
    "    key, subkey = random.split(key)\n",
    "    selected = random.categorical(subkey, logits, axis=-1)[:, None]\n",
    "\n",
    "    # Map selected back to original logit indices\n",
    "    next_token_id = jnp.take_along_axis(indices, selected, axis=-1)\n",
    "\n",
    "    return next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ll.checkpoint.load_config(\"Llama3.2-3b-Instruct\")\n",
    "tokenizer = ll.checkpoint.load_tokenizer(config)\n",
    "model = ll.model.create(config)\n",
    "executor = ThreadPoolExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1: No Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline1(prompts: Sequence[str]) -> Iterator[str]:\n",
    "\n",
    "    key = random.key(seed())\n",
    "\n",
    "    for content in prompts:\n",
    "        \n",
    "        prompt = ll.chat.render_prompt([Message(role=\"user\", content=content)])\n",
    "        token_ids, position_mask = tokenizer.encode(prompt)\n",
    "\n",
    "        x = token_ids\n",
    "        kvc = ll.kvc.create(config)\n",
    "        key, *subkeys = random.split(key, max_tokens+1)\n",
    "\n",
    "        for i in range(max_tokens):\n",
    "            \n",
    "            logits, kvc = transform(config, model, x, position_mask, kvc=kvc)\n",
    "            token_id = next_token_id(logits, key=subkeys[i])\n",
    "\n",
    "            yield tokenizer.decode(token_id)[0]\n",
    "\n",
    "            x = token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2: Queued\n",
    "\n",
    "Separate token encoding/decoding (CPU) from mapping token_ids to next_token_id (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline2(prompts: Sequence[str]) -> Iterator[str]:\n",
    "\n",
    "    input_queue = SimpleQueue()\n",
    "    output_queue = SimpleQueue()\n",
    "\n",
    "    def encoder():\n",
    "        \"\"\"Encode prompts as token ids.\"\"\"\n",
    "\n",
    "        for prompt in prompts:\n",
    "            prompt = ll.chat.render_prompt([Message(role=\"user\", content=prompt)])\n",
    "            token_ids, position_mask = tokenizer.encode(prompt)\n",
    "            \n",
    "            input_queue.put((token_ids, position_mask))\n",
    "\n",
    "        input_queue.put(None)\n",
    "\n",
    "    def transformer():\n",
    "        \"\"\"Predict next token id from input token ids.\"\"\"\n",
    "        key = random.key(seed())\n",
    "\n",
    "        while (inputs := input_queue.get()) is not None:\n",
    "            token_ids, position_mask = inputs\n",
    "            \n",
    "            x = token_ids\n",
    "            kvc = ll.kvc.create(config)\n",
    "            key, *subkeys = random.split(key, max_tokens+1)\n",
    "\n",
    "            for i in range(max_tokens):\n",
    "                logits, kvc = transform(config, model, x, position_mask, kvc=kvc)\n",
    "                token_id = next_token_id(logits, key=subkeys[i])\n",
    "\n",
    "                output_queue.put(token_id)\n",
    "\n",
    "                x = token_id\n",
    "            \n",
    "        output_queue.put(None)\n",
    "\n",
    "    # Launch background jobs\n",
    "    tasks = [\n",
    "        executor.submit(encoder),\n",
    "        executor.submit(transformer),\n",
    "    ]\n",
    "\n",
    "    # Decode generated token ids\n",
    "    while (token_id := output_queue.get()) is not None:\n",
    "        yield tokenizer.decode(token_id)[0]\n",
    "    \n",
    "    wait(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(pipeline: Pipeline):\n",
    "    jax.clear_caches()\n",
    "\n",
    "    for _ in tqdm(pipeline([warmup_prompt]), desc=\"Warmup\", total=max_tokens, leave=False):\n",
    "        pass\n",
    "\n",
    "\n",
    "def run(pipeline: Pipeline):\n",
    "\n",
    "    with ll.render.token_view() as tv:\n",
    "        for token in pipeline(prompts):\n",
    "            tv.add_token(token)\n",
    "    \n",
    "    return tv.tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d92d2fb4424db09b6505e35d8ce7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pipelines:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c946e4a89ac34819b5cad674acf48f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1b98bc11be40a0947b16f924cace0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff6873361284179a14a0903a2d0d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23ad8f70a1c40958adf19cceaba1ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipelines = [\n",
    "    pipeline1,\n",
    "    pipeline2,\n",
    "]\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for i, pipeline in enumerate(tqdm(pipelines, desc=\"Pipelines\")):\n",
    "    \n",
    "    warmup(pipeline)\n",
    "\n",
    "    tps = run(pipeline)\n",
    "\n",
    "    metrics.append({\"pipeline\": i, \"tps\": tps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='pipeline', ylabel='tps'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data, x=\"pipeline\", y=\"tps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
