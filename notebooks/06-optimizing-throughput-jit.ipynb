{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Throughput: JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown the average throughput of llama-jax to be 10 tps short of Ollama (25 vs. 35 running on 64GB M1 MBP). The goal of this series of optimization experiments is to see how closely we can match Ollama's throughput.\n",
    "\n",
    "## JIT\n",
    "\n",
    "Jax's just in time compiler is key optimization tool. But where do we apply it? Here we compare tps of jit at the very top level, vs second level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterator\n",
    "from concurrent.futures import wait,ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from queue import SimpleQueue\n",
    "from time import time_ns as seed\n",
    "\n",
    "import jax\n",
    "from jax import Array, numpy as jnp, random\n",
    "from jax.nn import softmax\n",
    "import ollama\n",
    "from pandas import DataFrame\n",
    "from pydantic import BaseModel\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import llama_jax as ll\n",
    "from llama_jax.chat import Message\n",
    "from llama_jax.checkpoint import ModelConfig\n",
    "from llama_jax.model import Model\n",
    "from llama_jax.kvc import KVCache\n",
    "from llama_jax.tools import default_arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prompts = 20\n",
    "max_tokens = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptPool(BaseModel):\n",
    "    prompts: list[str]\n",
    "\n",
    "prompt = ll.tools.prompt(\n",
    "    f\"\"\"\n",
    "    Generate {n_prompts} LLM prompts. Each prompt should pose an interesting question about math and science in 3 to 6 words.\n",
    "    Your response must be formatted as JSON.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\", \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "    format=PromptPool.model_json_schema(),\n",
    ")\n",
    "\n",
    "prompt_pool = PromptPool.model_validate_json(response.message.content)\n",
    "prompts = prompt_pool.prompts\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_prompt = \"Why is the sky blue?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(\n",
    "    config: ModelConfig,\n",
    "    state: Model,\n",
    "    token_ids: Array,\n",
    "    position_mask: Array,\n",
    "    *,\n",
    "    kvc: KVCache | None = None,\n",
    ") -> Array | tuple[Array, KVCache]:\n",
    "    \"\"\"Transform token_ids into next token logits.\"\"\"\n",
    "\n",
    "    # Remember if cache was provided\n",
    "    external_cache = kvc is not None\n",
    "\n",
    "    # Defaults\n",
    "    kvc = default_arg(kvc, default_factory=partial(ll.kvc.create, config))\n",
    "\n",
    "    # Sanity check\n",
    "    assert token_ids.ndim == 2\n",
    "\n",
    "    # Map tokens to embeddings\n",
    "    x = ll.embeddings.forward(config, state.embeddings, token_ids)\n",
    "\n",
    "    # Create mask\n",
    "    mask = ll.attention.attention_mask(config, position_mask)\n",
    "\n",
    "    # Create mutable kv cache\n",
    "    kvc_layers = list(kvc)\n",
    "\n",
    "    # Apply layers\n",
    "    for i, layer in enumerate(state.layers):\n",
    "        x, kvc_layers[i] = ll.layer.forward(config, layer, state.rope, mask, x, kvc_layers[i])\n",
    "\n",
    "    # Convert kv caches back into immutable sequence\n",
    "    kvc = KVCache(kvc_layers)\n",
    "\n",
    "    # Apply head\n",
    "    x = ll.head.forward(config, state.head, x, position_mask)\n",
    "\n",
    "    # Return updated cache if provided\n",
    "    if external_cache:\n",
    "        return x, kvc\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def next_token_id(\n",
    "    logits: Array,\n",
    "    *,\n",
    "    key: Array | None = None,\n",
    "    temperature: float | None = None,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    ") -> Array:\n",
    "    \"\"\"Select next token id using temperature, top k, and top p sampling.\"\"\"\n",
    "\n",
    "    # Temperature\n",
    "    # -----------\n",
    "\n",
    "    # Defaults\n",
    "    temperature = default_arg(temperature, 0.6)\n",
    "\n",
    "    # If temperature is 0, return the top token\n",
    "    if temperature == 0:\n",
    "        return jnp.argmax(logits, axis=-1, keepdims=True)\n",
    "\n",
    "    # Apply temperature\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Ranking\n",
    "    # -------\n",
    "\n",
    "    # Sort logits in descending order, maintaining original indices\n",
    "    indices = jnp.argsort(logits, axis=-1, descending=True)\n",
    "\n",
    "    # Top K\n",
    "    # -----\n",
    "\n",
    "    # Defaults\n",
    "    top_k = default_arg(top_k, 50)\n",
    "\n",
    "    # Apply top k to entire batch at once\n",
    "    indices = indices[:, :top_k]\n",
    "    logits = jnp.take_along_axis(logits, indices, axis=-1)\n",
    "\n",
    "    # Top P\n",
    "    # -----\n",
    "\n",
    "    # Defaults\n",
    "    top_p = default_arg(top_p, 0.9)\n",
    "\n",
    "    # Convert remaining logits to probabilities\n",
    "    probs = softmax(logits, axis=-1)\n",
    "\n",
    "    # Find index where cumulative sum of probs exceeds p\n",
    "    cumulative_mask = probs.cumsum(axis=-1) <= top_p\n",
    "    cutoff = jnp.sum(cumulative_mask, axis=-1, keepdims=True)\n",
    "\n",
    "    # Calculate mask for indicies <= cutoff\n",
    "    mask = jnp.broadcast_to(jnp.arange(logits.shape[-1]), logits.shape) <= cutoff\n",
    "\n",
    "    # Zero out logits above cutoff\n",
    "    logits = jnp.where(mask, logits, 0)\n",
    "\n",
    "    # Random Selection\n",
    "    # ----------------\n",
    "\n",
    "    assert key is not None\n",
    "\n",
    "    # Randomly choose from remaining logits\n",
    "    key, subkey = random.split(key)\n",
    "    selected = random.categorical(subkey, logits, axis=-1)[:, None]\n",
    "\n",
    "    # Map selected back to original logit indices\n",
    "    next_token_id = jnp.take_along_axis(indices, selected, axis=-1)\n",
    "\n",
    "    return next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ll.checkpoint.load_config(\"Llama3.2-3b-Instruct\")\n",
    "tokenizer = ll.checkpoint.load_tokenizer(config)\n",
    "model = ll.model.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generator = Callable[[str], Iterator[str]]\n",
    "\n",
    "def run(pipeline_name: str, generator: Generator):\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        with ll.render.token_view(prompt=prompt) as tv:\n",
    "            for token in generator(prompt):\n",
    "                tv.add_token(token)\n",
    "        \n",
    "        metrics.append({\"pipeline\": pipeline_name, \"prompt\": i, \"tps\": tv.tps})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset metrics\n",
    "pipeline_metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1: JIT Top Level\n",
    "\n",
    "Compile pipeline as one giant jit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"pipeline1\"\n",
    "\n",
    "@partial(jax.jit, static_argnames=\"config\")\n",
    "def predict(config, model, x, position_mask, kvc, key):\n",
    "    logits, kvc = transform(config, model, x, position_mask, kvc=kvc)\n",
    "    token_id = next_token_id(logits, key=key)\n",
    "\n",
    "    return token_id, kvc\n",
    "\n",
    "def pipeline(content: str) -> Iterator[str]:\n",
    "\n",
    "    key = random.key(seed())\n",
    "\n",
    "    prompt = ll.chat.render_prompt([Message(role=\"user\", content=content)])\n",
    "    token_ids, position_mask = tokenizer.encode(prompt)\n",
    "\n",
    "    x = token_ids\n",
    "    kvc = ll.kvc.create(config)\n",
    "    key, *subkeys = random.split(key, max_tokens+1)\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "        \n",
    "        token_id, kvc = predict(config, model, x, position_mask, kvc, subkeys[i])\n",
    "\n",
    "        yield tokenizer.decode(token_id)[0]\n",
    "\n",
    "        x = token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.clear_caches()\n",
    "\n",
    "for token in tqdm(pipeline(warmup_prompt), desc=\"Warmup\", unit_scale=True):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = run(pipeline_name, pipeline)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_metrics += metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2: Transform / Next Token JIT\n",
    "\n",
    "Apply jit to `transform` and `next_token_id` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"pipeline2\"\n",
    "\n",
    "_transform = jax.jit(transform, static_argnames=\"config\")\n",
    "_next_token_id = jax.jit(next_token_id)\n",
    "\n",
    "def pipeline(content: str) -> Iterator[str]:\n",
    "\n",
    "    key = random.key(seed())\n",
    "\n",
    "    prompt = ll.chat.render_prompt([Message(role=\"user\", content=content)])\n",
    "    token_ids, position_mask = tokenizer.encode(prompt)\n",
    "\n",
    "    x = token_ids\n",
    "    kvc = ll.kvc.create(config)\n",
    "    key, *subkeys = random.split(key, max_tokens+1)\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "        \n",
    "        logits, kvc = _transform(config, model, x, position_mask, kvc=kvc)\n",
    "        token_id = _next_token_id(logits, key=subkeys[i])\n",
    "\n",
    "        yield tokenizer.decode(token_id)[0]\n",
    "\n",
    "        x = token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.clear_caches()\n",
    "\n",
    "for token in tqdm(pipeline(warmup_prompt), desc=\"Warmup\", unit_scale=True):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = run(pipeline_name, pipeline)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_metrics += metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(pipeline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data, x=\"tps\", hue=\"pipeline\", multiple=\"dodge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline2\n",
    "data[data.pipeline == \"pipeline2\"].tps.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline3\n",
    "data[data.pipeline == \"pipeline3\"].tps.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
