{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb350ef-d055-4961-82c4-bda4cee91875",
   "metadata": {},
   "source": [
    "# MMLU Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2791cc-70b9-423c-b3b4-4aad5cc30f5b",
   "metadata": {},
   "source": [
    "[Massive Multitask Language Understanding (MMLU)](https://github.com/hendrycks/test) is a popular benchmark for evaluating language models' world knowledge and problem solving abilities. The MMLU dataset contains 14,042 multiple choice questions (MCQs) from 57 categories including mathematics, history, biology, and business. Each question has 4 options (A, B, C, D) and one correct answer. In addition, each category includes 5 example questions designed for few shot experiments. When MMLU was first published in 2020, only the largest GPT models could do better than random guessing. By 2024, multiple models from OpenAI, Anthropic, Meta, and Tencent have all published MMLU accuracies over 88%.\n",
    "\n",
    "In this experiment, we'll measure Llama performance against MMLU ourselves. Our goal is to recreate Meta's published MMLU benchmark scores:\n",
    "\n",
    "* MMLU of Llama 3.2 3B of 58% ([MODEL CARD](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449436a-8739-4db8-8075-b748a9503dd8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22db0fd-1533-4596-a128-569f145eb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import Formatter, StreamHandler\n",
    "import os\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "import sys\n",
    "from time import perf_counter_ns as timer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "import llama_jax as ll\n",
    "from llama_jax.benchmarks.mmlu import (\n",
    "    display_questions,\n",
    "    download_dataset, \n",
    "    load_dataset,\n",
    "    evaluate_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f86ceb5-e601-492c-ac53-1b0a0331d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure\n",
    "datasets_path = Path(os.environ[\"PROJECT_ROOT\"]) / \"build\" / \"datasets\"\n",
    "mmlu_dataset_path = datasets_path / \"mmlu\"\n",
    "\n",
    "# formatter = Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\")\n",
    "# handler = StreamHandler(stream=sys.stderr)\n",
    "# handler.setFormatter(formatter)\n",
    "# logging.root.addHandler(handler)\n",
    "# logging.root.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892a410-df36-4c3e-a9af-9360d3ec66bd",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae9f228-637e-44c0-832b-52c36952e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(mmlu_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4a43a-1caf-4ab8-b48e-de18d40c23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(mmlu_dataset_path)\n",
    "print(f\"Loaded {len(dataset.questions)} questions, {len(dataset.examples)} examples, {len(dataset.categories)} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3475474-6d21-49ff-aef8-1561f6082841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample\n",
    "display_questions(dataset.questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63adb7ed-bf61-43b2-97c4-abf699c5b5f5",
   "metadata": {},
   "source": [
    "# Zero-Shot, Sampled\n",
    "\n",
    "Before we run the end to end MMLU benchmark, this first stage will measure the accuracy on a small sample with no examples (0-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322e9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Llama3.2-3B-Instruct\"\n",
    "n_iterations = 3\n",
    "n_questions = 128\n",
    "n_shots = 0\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704eaf89-660e-4565-a731-037bd1ecec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mmlu generator from checkpoint\n",
    "config = ll.checkpoint.load_config(checkpoint, max_tokens=1024)\n",
    "generator = ll.benchmarks.mmlu.generator(config, n_shots=n_shots, examples=dataset.examples, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092278e2-1ff7-45a3-8bf9-63ee2f794167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compile model\n",
    "next(generator(sample(dataset.questions, k=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddaeef-1d21-4ee2-ab16-bc2a9d9a0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timer()\n",
    "\n",
    "scores = []\n",
    "for _ in trange(n_iterations, desc=\"Iterations\"):\n",
    "\n",
    "    # Randomly sample questions\n",
    "    questions = sample(dataset.questions, k=n_questions)\n",
    "\n",
    "    # Track progress\n",
    "    progress = tqdm(total=n_questions, desc=\"Questions\", leave=False)\n",
    "    \n",
    "    score = evaluate_generator(\n",
    "        generator,\n",
    "        questions=questions,\n",
    "        progress=progress,\n",
    "    )\n",
    "    scores.append(score)\n",
    "\n",
    "duration = ((timer() - start_time) / 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c586a3-df9b-4125-bec6-5a92cb76e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = duration / (n_iterations * n_questions)\n",
    "print(f\"Average {t:0.2f} s/q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54849bb8-06c2-47d4-a62b-1b4344a16780",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores)\n",
    "plt.ylabel(\"MMLU Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388ad1c-ab5d-4657-9e81-689332d35fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
